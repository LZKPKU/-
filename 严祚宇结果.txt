xgboost: "learning_rate":[0.1],
    "n_estimators":[1000],
    "silent":[True],
    "max_depth":[5],
    "colsample_bytree":[0.5],
    "objective":["reg:linear"],
    "subsample":[0.7] 
    mse:21286

AdaBoostRegressor(base_estimator=None, learning_rate=1, loss='square',n_estimators=2000, random_state=None) mse:30000+,但加进去之后有将近100的进步

BaggingRegressor(base_estimator=None, bootstrap=True,bootstrap_features=False, max_features=0.5, max_samples=1.0,n_estimators=200, n_jobs=1, oob_score=False, random_state=None,verbose=0, warm_start=True) mse:26600

ExtraTreesRegressor(bootstrap=False, criterion='mse', max_depth=None,
          max_features='auto', max_leaf_nodes=None,
          min_impurity_decrease=0.0, min_impurity_split=None,
          min_samples_leaf=1, min_samples_split=2,
          min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,
          oob_score=False, random_state=None, verbose=0, warm_start=False) mse：26000左右

RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=100,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,
           oob_score=False, random_state=None, verbose=0, warm_start=True)







